{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TobyChen320/Awesome-Profile-README-templates/blob/master/Revenue_Cycle_Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enXb1FKUDBHR"
      },
      "outputs": [],
      "source": [
        "# This is an attempt to reformat a raw remit text file to the format we normally see when we pass the file into \"HIPAA to Excel\"\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/NYCBS_HOSP_REMIT_1199_NATIONAL_BEN_FUND_03273OU101_PST.DAT\", delimiter ='\\t')\n",
        "df.to_excel('Remit_Test.xlsx', index =False)\n",
        "df_excel = pd.read_excel('Remit_Test.xlsx')\n",
        "df_excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD1WILCN8_w7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlfQ2PY68n_7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"WO.csv\")\n",
        "unique_counts = df['Comments'].value_counts().reset_index()\n",
        "unique_counts.columns = ['Unique_values', 'Count']\n",
        "unique_counts.to_csv('100%_WO_Counts.csv', index =False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz3W3KSQu0rD"
      },
      "outputs": [],
      "source": [
        "# This block of code will append your data in order. You just need to change the sheet name and column names.\n",
        "import os\n",
        "import pandas as pd\n",
        "directory = 'EKG'\n",
        "\n",
        "excel_files = sorted([filename for filename in os.listdir(directory) if filename.startswith('EKG')])\n",
        "\n",
        "combined_data_sheet1 = pd.DataFrame()\n",
        "combined_data_sheet2 = pd.DataFrame()\n",
        "\n",
        "for filename in excel_files:\n",
        "  file_path = os.path.join(directory, filename)\n",
        "  xls = pd.ExcelFile(file_path)\n",
        "  for sheet_name in xls.sheet_names:\n",
        "    df = pd.read_excel(xls, sheet_name = sheet_name)\n",
        "    if sheet_name == 'CARD_EKG_NM_C_R_Statuses':\n",
        "      df['Source_File'] = filename\n",
        "      combined_data_sheet1 = combined_data_sheet1.append(df, ignore_index = True)\n",
        "    elif sheet_name == 'CARD_EKG_NM_All_Statuses':\n",
        "      df['Source_File'] = filename\n",
        "      combined_data_sheet2 = combined_data_sheet2.append(df, ignore_index = True)\n",
        "\n",
        "\n",
        "with pd.ExcelWriter('EKG_Combined.xlsx', engine = 'openpyxl') as writer:\n",
        "  combined_data_sheet1.to_excel(writer, sheet_name = 'CARD_EKG_NM_C_R_Statuses', index = False)\n",
        "  combined_data_sheet2.to_excel(writer, sheet_name = 'CARD_EKG_NM_All_Statuses')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se5xb0nEMP2N"
      },
      "outputs": [],
      "source": [
        "# This will sort multiple column ranges for you in excel.\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('NEW_Charge_Test.xlsx')\n",
        "df_sorted = df.sort_values(by=['UnitNumber',\t'AccountNumber', 'ServiceDateTime'])\n",
        "df_sorted.to_excel('sorted_new_charge.xlsx', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK-yU5Kj8cL4"
      },
      "outputs": [],
      "source": [
        "# Appends the texts inside each file into 1 file.\n",
        "import fileinput\n",
        "import glob\n",
        "\n",
        "file_list = glob.glob(\"*.BAK\")\n",
        "\n",
        "with open('Claims2022.txt', 'w') as file:\n",
        "    input_lines = fileinput.input(file_list, encoding='cp1252')\n",
        "    file.writelines(input_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UsKM6pMPZCy"
      },
      "outputs": [],
      "source": [
        "# Change UCRN based on Row Update Time\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"Dyann 99202_march 2024 to march 5 2025 visits Nate file.xlsx\")\n",
        "\n",
        "# Ensure RowUpdateDateTime is a datetime type\n",
        "# df[\"RowUpdateDateTime (BarBillsUB92Claims)\"] = pd.to_datetime(df[\"RowUpdateDateTime (BarBillsUB92Claims)\"])\n",
        "\n",
        "# Get the most recent row for each AccountNumber\n",
        "result = df.loc[df.groupby(\"AccountNumber\")[\"AccountQueryID\"].idxmax()]\n",
        "\n",
        "result.to_excel(\"Test.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6VzILMvOMnI"
      },
      "outputs": [],
      "source": [
        "# Adhoc Pneumonia python format code.\n",
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.read_csv(\"Pneumonia.csv\")\n",
        "\n",
        "grouped = df.groupby(['Account Number', 'Admit Date Time', 'Discharge Date Time', 'Final Drg'])['Diagnosis'].apply(list).reset_index()\n",
        "\n",
        "# Create a new DataFrame with the desired format\n",
        "max_diagnoses = grouped['Diagnosis'].apply(len).max()\n",
        "diagnosis_columns = [f'Diagnosis {i+1}' for i in range(max_diagnoses)]\n",
        "\n",
        "# Expand the list of diagnoses into separate columns\n",
        "expanded_diagnoses = pd.DataFrame(grouped['Diagnosis'].tolist(), columns=diagnosis_columns)\n",
        "\n",
        "# Combine the expanded diagnoses with the original grouped DataFrame\n",
        "result = pd.concat([grouped.drop(columns=['Diagnosis']), expanded_diagnoses], axis=1)\n",
        "\n",
        "# Reorder columns to match the desired output\n",
        "column_order = ['Account Number', 'Admit Date Time'] + diagnosis_columns + ['Discharge Date Time', 'Final Drg']\n",
        "result = result[column_order]\n",
        "\n",
        "result.to_csv(\"Pneumonia 4th Quarter Not In DRG.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvkxRAP7LsNn"
      },
      "source": [
        "This excel formula combines 2 column's values with a comma seperating them only if there are 2 values. Otherwise just returns the one. (Currently meant for TPA Charge Detail File)\n",
        "\n",
        "=IF(AND(J2<>\"\", K2<>\"\"), J2 & \", \" & K2, J2 & K2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This formula takes each value in the cell and wraps it in single apostrophes while seperating them with a comma\n",
        "\n",
        "=\"'\" & TEXTJOIN(\"', '\", TRUE, A1:A2907) & \"'\""
      ],
      "metadata": {
        "id": "u4GsGbqrqcUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes rows with null values for service to and service from date fields\n",
        "# Working Claim Summary File\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "df = pd.read_csv(\"Claim Summary File Final.csv\")\n",
        "\n",
        "# Ensure 'BillNumberID (BarBills)' and 'Type Of Bill' are numeric for comparison\n",
        "df['BillNumberID (BarBills)'] = pd.to_numeric(df['BillNumberID (BarBills)'], errors='coerce')\n",
        "\n",
        "# Step 1: Handle null values in 'BillNumberID (BarBills)' by replacing them with a very low value\n",
        "df['BillNumberID (BarBills)'].fillna(-float('inf'), inplace=True)\n",
        "\n",
        "# Convert 'Health Plan ID Number' to numeric and integer type to avoid decimal issues\n",
        "df['Health Plan ID Number'] = pd.to_numeric(df['Health Plan ID Number'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Identify the row with the maximum 'BillNumberID (BarBills)' for each 'Account Number'\n",
        "max_qid_rows = df.loc[df.groupby('Encounter ID')['BillNumberID (BarBills)'].idxmax()]\n",
        "\n",
        "# Restore null values in 'BillNumberID (BarBills)' where applicable\n",
        "max_qid_rows.loc[max_qid_rows['BillNumberID (BarBills)'] == -float('inf'), 'BillNumberID (BarBills)'] = None\n",
        "\n",
        "# Step 2: Consolidate all fields for each 'Account Number'\n",
        "# Exclude the fields that should not be consolidated ('Type Of Bill', 'Acct Num QID', etc.)\n",
        "fields_to_exclude = ['BillNumberID (BarBills)']\n",
        "consolidated_data = df.groupby('Encounter ID').agg(\n",
        "    lambda x: ', '.join(sorted(x.dropna().astype(str).unique()))\n",
        ")\n",
        "\n",
        "# Step 3: Replace consolidated values for excluded fields with values from max_qid_rows\n",
        "for field in fields_to_exclude:\n",
        "    consolidated_data[field] = max_qid_rows.set_index('Encounter ID')[field]\n",
        "\n",
        "# Step 4: Merge the consolidated fields into a single DataFrame\n",
        "result_df = max_qid_rows[['Encounter ID']].merge(\n",
        "    consolidated_data,\n",
        "    on='Encounter ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Reset the index for clarity\n",
        "result_df = result_df.reset_index(drop=True)\n",
        "\n",
        "# Step 5: Filter the DataFrame to only include the desired fields\n",
        "desired_fields = [\n",
        "    'Facility Name', 'Encounter ID', 'Patient Type', 'Type of Bill',\n",
        "    'Type of Admission', 'Service From Date', 'Service To Date', 'Payor Plan Name',\n",
        "    'Health Plan ID Number', 'Financial Class ID', 'Date of Birth', 'Sex',\n",
        "    'Discharge status', 'Total Charges', 'Final Drg', 'MS DRG', 'Diagnosis Codes', 'Procedure Codes'\n",
        "]\n",
        "\n",
        "result_df = result_df[desired_fields]\n",
        "\n",
        "# Step 6: Drop rows with blank Service From Date or Service To Date\n",
        "result_df = result_df[\n",
        "    (result_df['Service From Date'] != '') &\n",
        "    (result_df['Service To Date'] != '')\n",
        "]\n",
        "\n",
        "# Save the resulting DataFrame to a new CSV file\n",
        "result_df.to_csv(\"Claim Summary Month Sample.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRY87JJgkaQT",
        "outputId": "58b4fd2f-8ba2-4192-b1a2-6864fbc27dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-39f240a4c187>:5: DtypeWarning: Columns (4,15,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"Claim Summary File Final.csv\")\n",
            "<ipython-input-3-39f240a4c187>:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['BillNumberID (BarBills)'].fillna(-float('inf'), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD\n",
        "# Working Claim Summary File\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "df = pd.read_csv(\"Claim Summary File Final.csv\")\n",
        "\n",
        "# Ensure 'BillNumberID (BarBills)' and 'Type Of Bill' are numeric for comparison\n",
        "df['BillNumberID (BarBills)'] = pd.to_numeric(df['BillNumberID (BarBills)'], errors='coerce')\n",
        "\n",
        "# Step 1: Handle null values in 'BillNumberID (BarBills)' by replacing them with a very low value\n",
        "df['BillNumberID (BarBills)'].fillna(-float('inf'), inplace=True)\n",
        "\n",
        "# Convert 'Health Plan ID Number' to numeric and integer type to avoid decimal issues\n",
        "df['Health Plan ID Number'] = pd.to_numeric(df['Health Plan ID Number'], errors='coerce').astype('Int64')\n",
        "\n",
        "# Identify the row with the maximum 'BillNumberID (BarBills)' for each 'Account Number'\n",
        "max_qid_rows = df.loc[df.groupby('Encounter ID')['BillNumberID (BarBills)'].idxmax()]\n",
        "\n",
        "# Restore null values in 'BillNumberID (BarBills)' where applicable\n",
        "max_qid_rows.loc[max_qid_rows['BillNumberID (BarBills)'] == -float('inf'), 'BillNumberID (BarBills)'] = None\n",
        "\n",
        "# Step 2: Consolidate all fields for each 'Account Number'\n",
        "# Exclude the fields that should not be consolidated ('Type Of Bill', 'Acct Num QID', etc.)\n",
        "fields_to_exclude = ['BillNumberID (BarBills)']\n",
        "consolidated_data = df.groupby('Encounter ID').agg(\n",
        "    lambda x: ', '.join(sorted(x.dropna().astype(str).unique()))\n",
        ")\n",
        "\n",
        "# Step 3: Replace consolidated values for excluded fields with values from max_qid_rows\n",
        "for field in fields_to_exclude:\n",
        "    consolidated_data[field] = max_qid_rows.set_index('Encounter ID')[field]\n",
        "\n",
        "# Step 4: Merge the consolidated fields into a single DataFrame\n",
        "result_df = max_qid_rows[['Encounter ID']].merge(\n",
        "    consolidated_data,\n",
        "    on='Encounter ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Reset the index for clarity\n",
        "result_df = result_df.reset_index(drop=True)\n",
        "\n",
        "# Step 5: Filter the DataFrame to only include the desired fields\n",
        "desired_fields = [\n",
        "    'Facility Name', 'Encounter ID', 'Patient Type', 'Type of Bill',\n",
        "    'Type of Admission', 'Service From Date', 'Service To Date', 'Payor Plan Name',\n",
        "    'Health Plan ID Number', 'Financial Class ID', 'Date of Birth', 'Sex',\n",
        "    'Discharge status', 'Total Charges', 'Final Drg', 'MS DRG', 'Diagnosis Codes', 'Procedure Codes'\n",
        "]\n",
        "\n",
        "result_df = result_df[desired_fields]\n",
        "\n",
        "# Save the resulting DataFrame to a new CSV file\n",
        "result_df.to_csv(\"Claim Summary Month Sample.csv\", index=False)\n",
        "\n",
        "print(\"Consolidation complete! Saved as 'Claim Summary Month Sample.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "9a_lJcTa3l8J",
        "outputId": "7d098c9b-3e15-4140-becb-d6a6f485ab11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'BillNumberID (BarBills)'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'BillNumberID (BarBills)'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-918a8c43ca83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Ensure 'BillNumberID (BarBills)' and 'Type Of Bill' are numeric for comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BillNumberID (BarBills)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BillNumberID (BarBills)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 1: Handle null values in 'BillNumberID (BarBills)' by replacing them with a very low value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'BillNumberID (BarBills)'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nGuJlq_kTyf",
        "outputId": "26c0fe95-e007-4188-ce89-84b95538aab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e777fbde54c2>:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['EffectiveDateTime (DBarProcAltCodeEffectDates)'] = pd.to_datetime(df['EffectiveDateTime (DBarProcAltCodeEffectDates)'])\n"
          ]
        }
      ],
      "source": [
        "# Charge Detail File Step 1\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"Charge Detail File.csv\")\n",
        "\n",
        "# Convert 'EffectiveDateTime' to datetime format\n",
        "df['EffectiveDateTime (DBarProcAltCodeEffectDates)'] = pd.to_datetime(df['EffectiveDateTime (DBarProcAltCodeEffectDates)'])\n",
        "\n",
        "# Separate rows where 'EffectiveDateTime' is NaT before filtering\n",
        "nat_rows = df[df['EffectiveDateTime (DBarProcAltCodeEffectDates)'].isna()]\n",
        "\n",
        "# Keep rows with the latest effective date for each 'ProcedureID (DBarProcedures)'\n",
        "filtered_df = df[\n",
        "    df['EffectiveDateTime (DBarProcAltCodeEffectDates)'].notna() &\n",
        "    (df.groupby('CDM Item Code')['EffectiveDateTime (DBarProcAltCodeEffectDates)']\n",
        "     .transform('max') == df['EffectiveDateTime (DBarProcAltCodeEffectDates)'])\n",
        "]\n",
        "\n",
        "# Combine the filtered rows with those that had NaT in 'EffectiveDateTime'\n",
        "filtered_df = pd.concat([filtered_df, nat_rows], ignore_index=True)\n",
        "\n",
        "# Ensure 'Code' column values are treated as strings\n",
        "filtered_df['HCPCS or CPT Code'] = filtered_df['HCPCS or CPT Code'].astype(str)\n",
        "\n",
        "# Function to create the 'Modifier(s)' column\n",
        "def extract_modifiers(code):\n",
        "    if len(code) <= 5:\n",
        "        return \"\"  # Blank if only 5 characters\n",
        "    elif len(code) == 7:\n",
        "        return code[5:7]  # Only last 2 characters if there are 7\n",
        "    elif len(code) > 7:\n",
        "        return f\"{code[5:7]},{code[7:9]}\"  # Characters 6-7 followed by a comma and next 2 characters\n",
        "    else:\n",
        "        return None  # Handle unexpected cases\n",
        "\n",
        "# Apply the function to create 'Modifier(s)' column\n",
        "filtered_df['Modifier'] = filtered_df['HCPCS or CPT Code'].apply(extract_modifiers)\n",
        "\n",
        "# Remove the modifiers from the 'Code' column (retain only the first 5 characters)\n",
        "filtered_df['HCPCS or CPT Code'] = filtered_df['HCPCS or CPT Code'].str[:5]\n",
        "\n",
        "# Drop 'Type ID' and 'EffectiveDateTime' columns\n",
        "filtered_df = filtered_df.drop(columns=['EffectiveDateTime (DBarProcAltCodeEffectDates)'])\n",
        "\n",
        "# Sort by 'Account Number' column in ascending order\n",
        "sorted_df = filtered_df.sort_values(by='Encounter ID', ascending=True)\n",
        "\n",
        "# Save the final DataFrame to a CSV file\n",
        "sorted_df.to_csv(\"Charge Detail File Month.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLql3SxwNxpp",
        "outputId": "ac4be180-bba0-4f41-b97e-95514ad99476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-56f79cdbac37>:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  filtered_df = df.groupby('CDM Item Code').apply(filter_rows).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "# Charge Detail File Final Step\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"Charge Detail File Month.csv\")\n",
        "\n",
        "# Sort by 'Encounter ID' and 'CDM Item Code' in ascending order\n",
        "df = df.sort_values(by=['Encounter ID', 'CDM Item Code'])\n",
        "\n",
        "# Group by 'CDM Item Code' and filter out rows where both CPT-4 and HCPCS exist\n",
        "def filter_rows(group):\n",
        "    if len(group) > 1 and 'CPT-4' in group['Type ID'].values and 'HCPCS' in group['Type ID'].values:\n",
        "        # If both CPT-4 and HCPCS exist for the same CDM Item Code, keep the CPT-4 row\n",
        "        return group[group['Type ID'] == 'CPT-4']\n",
        "    else:\n",
        "        # Otherwise, keep all rows\n",
        "        return group\n",
        "\n",
        "# Apply the filter to each group\n",
        "filtered_df = df.groupby('CDM Item Code').apply(filter_rows).reset_index(drop=True)\n",
        "\n",
        "# Drop the 'Type ID' column\n",
        "filtered_df = filtered_df.drop(columns=['Type ID'])\n",
        "\n",
        "# Select the desired columns\n",
        "final_df = filtered_df[['Encounter ID', 'CDM Item Code', 'Service Date', 'Revenue Code', 'Units', 'Total Charge', 'Cost center/Department', 'HCPCS or CPT Code', 'Modifier']]\n",
        "\n",
        "# Sort the final output by 'Encounter ID'\n",
        "final_df = final_df.sort_values(by=['Encounter ID'])\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "final_df.to_csv(\"Charge Detail File Month Sample.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpzUAHnmPPqr",
        "outputId": "dc6694d9-f97e-4634-f5fb-72d002641109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrepancies:\n",
            "Encounter ID  total_charge_claims  charge\n",
            "    VAT62552               120.01  120.00\n",
            "    VAU14852               763.01  763.00\n",
            "    VAU22472               400.01  400.00\n",
            "    VAU24202               873.01  873.00\n",
            "    VAU25025               210.01  210.00\n",
            "    VAU25027               610.01  610.00\n",
            "    VAU31310               763.01  763.00\n",
            "    VAU34029              1865.02 1865.01\n",
            "    VAU42238               447.00     NaN\n",
            "    VAU42241               302.02  302.00\n",
            "    VAU42245               453.01  453.00\n",
            "    VAU42258               453.01  453.00\n",
            "    VAU42260               453.01  453.00\n",
            "    VAU47509               302.01  302.00\n",
            "    VAU48733               610.01  610.00\n",
            "    VAU48955               102.01  102.00\n",
            "    VAU53926               692.00     NaN\n",
            "    VAU57977               323.01  323.00\n",
            "    VAU66284               127.21  127.20\n",
            "    VAU73094              4071.20 4070.40\n",
            "    VAU77846              4192.96 4192.95\n",
            "    VAU82284               120.01  120.00\n",
            "    VAU94939              8544.20 8544.19\n",
            "    VAV15022              3455.61 3455.60\n",
            "    VAV15175               292.01  292.00\n",
            "    VAV15176               292.01  292.00\n",
            "    VAV24423              3455.61 3455.60\n",
            "    VAV25279              9518.81 9518.80\n",
            "    VAV30212               763.00     NaN\n",
            "    VAV30453               400.00     NaN\n",
            "    VAV48566              1968.15 1968.14\n",
            "    VAV48674             15767.50     NaN\n",
            "    VAV51201              1385.29 1385.28\n",
            "    VAV51519               596.38  595.58\n",
            "    VAV52855               550.01  550.00\n",
            "    VAV52856               550.01  550.00\n",
            "    VAV53058               550.01  550.00\n",
            "    VAV53407               550.01  550.00\n",
            "    VAV53408               550.01  550.00\n",
            "    VAV53528               550.01  550.00\n",
            "    VAV54642              2862.40 2862.39\n",
            "    VAV56052               713.18  713.17\n",
            "    VAV57990               328.61  328.60\n",
            "    VAV59724              1033.39 1033.36\n",
            "    VAV61434               550.01  550.00\n",
            "    VAV63693               424.01  424.00\n",
            "    VAV64957              1822.01 1822.00\n",
            "    VAV66287               791.62  791.61\n",
            "    VAV70964              1751.88 1751.87\n",
            "    VAV74415               550.02  550.00\n",
            "    VAV74534               550.01  550.00\n",
            "    VAV75408               550.01  550.00\n",
            "    VAV75424               756.73  756.71\n",
            "    VAV75447              1085.31 1085.30\n",
            "    VAV75867               934.78 1484.78\n",
            "    VAV76341               550.01  550.00\n",
            "    VAV77311               550.00     NaN\n",
            "    VAV78553              1798.70 1798.68\n",
            "    VAV79733              3485.85 3485.84\n",
            "    VAV82064               550.82  550.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV files\n",
        "claims = pd.read_csv('Claim Summary Month Sample.csv') # Columns: Encounter ID, Total Charges\n",
        "charges = pd.read_csv('Charge Detail File Month Sample.csv') # Columns: Encounter ID, Total Charge\n",
        "\n",
        "# Rename columns for clarity and to avoid merge conflicts\n",
        "claims = claims.rename(columns={'Total Charges': 'total_charge_claims'})\n",
        "charges = charges.rename(columns={'Total Charge': 'charge'})\n",
        "\n",
        "# Sum charges per Encounter ID in the detail file\n",
        "charge_sum = charges.groupby('Encounter ID')['charge'].sum().reset_index()\n",
        "\n",
        "# Merge the two datasets\n",
        "merged = pd.merge(\n",
        "    claims,\n",
        "    charge_sum,\n",
        "    on='Encounter ID',\n",
        "    how='outer', # Checks for IDs missing in either file\n",
        "    suffixes=('_claims', '_charge')\n",
        ")\n",
        "\n",
        "# Flag discrepancies\n",
        "merged['discrepancy'] = False\n",
        "\n",
        "# Case 1: Totals don’t match (allow $0.01 tolerance for floating-point errors)\n",
        "mask = (\n",
        "    merged['total_charge_claims'].round(2) !=\n",
        "    merged['charge'].round(2)\n",
        ")\n",
        "merged.loc[mask, 'discrepancy'] = True\n",
        "\n",
        "# Case 2: Missing Encounter IDs in either file\n",
        "merged['discrepancy'] = merged['discrepancy'] | merged[['total_charge_claims', 'charge']].isna().any(axis=1)\n",
        "\n",
        "# Filter and display discrepancies\n",
        "discrepancies = merged[merged['discrepancy']][['Encounter ID', 'total_charge_claims', 'charge']]\n",
        "\n",
        "# Print results\n",
        "print(\"Discrepancies:\")\n",
        "print(discrepancies.to_string(index=False))\n",
        "\n",
        "# Optional: Save to CSV\n",
        "discrepancies.to_csv('discrepancy_report.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}